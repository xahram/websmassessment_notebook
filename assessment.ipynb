{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "import json\n",
    "import csv\n",
    "\n",
    "credentials = {}\n",
    "credentials[\"API_KEY\"] = \"kfMHh8BuUXMCQ9S57eHlZPtp3\"\n",
    "credentials[\"API_SECRET\"] = \"YBBa1e6czkDK30KOpdMPN20QCWao2M8ONvCodfnH8gWW1AOXIu\"\n",
    "credentials[\"ACCESS_TOKEN\"] = \"1501925600105218050-lOCbvbbF4Ikoy9ublZQi4NAiGsAN84\"\n",
    "credentials[\"ACCESS_TOKEN_SECRET\"] = \"cS5qbsBa9IvF6ORoKs6UZtGz5KbLe0nY9lINwWGeK35ao\"\n",
    "\n",
    "\n",
    "with open(\"credentials.json\", \"w\") as file:\n",
    "    json.dump(credentials, file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTHENTICATION OF TWITTER TOKENS OAUTH\n",
    "from twython import Twython\n",
    "import json\n",
    "\n",
    "credentials = {}\n",
    "with open(\"credentials.json\", \"r\") as file:\n",
    "    credentials = json.load(file)\n",
    "\n",
    "twitter = Twython(app_key=credentials[\"API_KEY\"],\n",
    "                 app_secret=credentials[\"API_SECRET\"], \n",
    "                 oauth_token=credentials[\"ACCESS_TOKEN\"],\n",
    "                 oauth_token_secret=credentials[\"ACCESS_TOKEN_SECRET\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Twitter_Utility:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.results = {\"statuses\":[]}\n",
    "\n",
    "    def process_tweet(self, data):\n",
    "        tweet = {}\n",
    "        tweet[\"created_at\"] = data[\"created_at\"]\n",
    "        tweet[\"location\"] = data[\"user\"][\"location\"]\n",
    "        tweet[\"followers_count\"] = data[\"user\"][\"followers_count\"]\n",
    "        tweet[\"text\"] = data[\"text\"]\n",
    "        tweet[\"client_platform\"] = data[\"source\"]\n",
    "        tweet[\"hashtags\"] = [hashtag[\"text\"] for hashtag in data[\"entities\"][\"hashtags\"]]\n",
    "        tweet[\"retweet_count\"] = data[\"retweet_count\"]\n",
    "        # tweet[\"favorited\"] = data[\"favorited\"]\n",
    "        tweet[\"favorite_count\"] = data[\"favorite_count\"]\n",
    "        if \"place\" in data and data[\"place\"] != None:\n",
    "            tweet[\"long\"] = data[\"place\"][\"bounding_box\"][\"coordinates\"][0][0][0]\n",
    "            tweet[\"lat\"] = data[\"place\"][\"bounding_box\"][\"coordinates\"][0][0][1]\n",
    "        else:\n",
    "            tweet[\"long\"] = None\n",
    "            tweet[\"lat\"] = None\n",
    "\n",
    "        return tweet\n",
    "\n",
    "\n",
    "    def csv_set_header(self, tweet):\n",
    "         with open(\"saved_tweets.csv\", \"a\", encoding=\"utf-8\",newline=\"\") as file:\n",
    "            header = [key for key in self.process_tweet(tweet)]\n",
    "            csv_writer = csv.writer(file)\n",
    "            csv_writer.writerow(header)\n",
    "        \n",
    "    def save_to_csv(self, tweet):\n",
    "        with open(\"saved_tweets.csv\", \"a\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "            csv_writer = csv.writer(file)\n",
    "            tweet_content = [tweet[key] for key in tweet]\n",
    "            csv_writer.writerow(tweet_content)\n",
    "            return tweet_content\n",
    "\n",
    "    def search_tweet(self, search_query,count,lang=\"en\",result_type=\"popular\"):\n",
    "        for i in range(40):\n",
    "            results = twitter.search(q=search_query, count= count,lang=lang, result_type=result_type)   \n",
    "            self.results[\"statuses\"].extend(results[\"statuses\"])\n",
    "        # self.results[\"statuses\"] = self.results[\"statuses\"].append(results[\"statuses\"])\n",
    "        # self.results[\"statuses\"] = self.results[\"statuses\"][0]\n",
    "        # self.results[\"statuses\"] = self.results[\"statuses\"][0]\n",
    "\n",
    "        \n",
    "        #print(\"multi list\", self.results)\n",
    "        return self.results\n",
    "        \n",
    "    \n",
    "    def save_tweets(self):\n",
    "        print(self.results[\"statuses\"])\n",
    "        with open(\"someran.json\", \"a\") as file:\n",
    "            json.dump(self.results[\"statuses\"][0], file)\n",
    "        if self.results.get(\"statuses\"):\n",
    "                self.csv_set_header(self.results[\"statuses\"][0])\n",
    "                for result in self.results[\"statuses\"]:\n",
    "                    tweet = self.process_tweet(result) \n",
    "                    self.save_to_csv(tweet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "twitter_util = Twitter_Utility()\n",
    "#twitter_util.search_tweet(\"Ukraine\", count=100)\n",
    "#twitter_util.save_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TWITTER WORLDWIDE TRENDS\n",
    "import pandas as pd\n",
    "\n",
    "def twitter_trends_url(url, id):\n",
    "    url = twitter.construct_api_url(api_url=url, id = id)\n",
    "    print(url)\n",
    "    return url\n",
    "\n",
    "response = twitter._request(url= twitter_trends_url(\"https://api.twitter.com/1.1/trends/place.json\",id=23424977),\n",
    "                    json_encoded=True,\n",
    "                    method=\"GET\")\n",
    "\n",
    "with open(\"worldtrends.json\", \"w\") as file:\n",
    "        json.dump(response, file)\n",
    "\n",
    "\n",
    "top_15_trends_list = list(response[0][\"trends\"])[0:15]\n",
    "\n",
    "top_15_trends_list\n",
    "df = pd.DataFrame(columns=[\"name\",\"tweet_volume\",\"url\",\"query\"])\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/10715965/create-a-pandas-dataframe-by-appending-one-row-at-a-time\n",
    "\n",
    "for index in range(len(top_15_trends_list)):\n",
    "    df.loc[index] = [top_15_trends_list[index][\"name\"], \n",
    "                        top_15_trends_list[index][\"tweet_volume\"], \n",
    "                        top_15_trends_list[index][\"url\"],\n",
    "                        top_15_trends_list[index][\"query\"]\n",
    "                        ] \n",
    "\n",
    "\n",
    "\n",
    "df\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### BARPLOT FOR WORLDWIDE TRENDS\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# type(df[\"tweet_volume\"])\n",
    "df = df.dropna()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sorted_df = df.sort_values(['tweet_volume'], ascending=False).reset_index(drop=True)\n",
    "sns.barplot(data=sorted_df,\n",
    "             x=\"name\", \n",
    "             y=\"tweet_volume\",\n",
    "             order=sorted_df.name)\n",
    "plt.xlabel(\"USA Twitter Trends\", size= 15)             \n",
    "plt.ylabel(\"Tweet Count\", size= 15)             \n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trend_url = df[df[\"name\"]==\"Ted Cruz\"].url.to_string(index=False)\n",
    "#trend_tweets = twitter_util.search_tweet(sorted_df.iat[0,0], count=10)\n",
    "\n",
    "trend_url = df[df[\"name\"]==\"Ted Cruz\"].url.to_string(index=False)\n",
    "# trend_url\n",
    "\n",
    "top_trend_on_twitter_query = sorted_df.iat[0,0]\n",
    "trend_tweets = twitter_util.search_tweet(top_trend_on_twitter_query,count=100)\n",
    "twitter_util.results = trend_tweets\n",
    "print(\"sdfasdfsadsf\",top_trend_on_twitter_query)\n",
    "twitter_util.save_tweets()\n",
    "\n",
    "\n",
    "# with open(\"ran.json\", \"a\") as file:\n",
    "#     json.dump(trend_tweets,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# PIECHART FOR DEVICES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.font_manager\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['text.color'] = '#909090'\n",
    "plt.rcParams['axes.labelcolor']= '#909090'\n",
    "plt.rcParams['xtick.color'] = '#909090'\n",
    "plt.rcParams['ytick.color'] = '#909090'\n",
    "plt.rcParams['font.size']=12\n",
    "\n",
    "\n",
    "# labels = [w for w,k in itertools.groupby(twitter_client_system , lambda x : x)]\n",
    "from pathlib import Path\n",
    "import re\n",
    "tweets_trending_file_path = Path() / \"saved_tweets.csv\"\n",
    "trending_tweets = pd.read_csv(tweets_trending_file_path)\n",
    "\n",
    "\n",
    "import re\n",
    "client_system = trending_tweets[\"client_platform\"].tolist()\n",
    "\n",
    "#print(client_system)\n",
    "twitter_client_system = []\n",
    "for i in client_system:\n",
    "    match = re.search(r'Android|iPhone|Web App|Tweetdeck|Tweetbot|TweetDeck',i)\n",
    "    if match is not None:\n",
    "      device = match.group()\n",
    "      twitter_client_system.append(device)\n",
    "\n",
    "print(twitter_client_system) \n",
    "\n",
    "\n",
    "counts = dict()\n",
    "for i in twitter_client_system:\n",
    "  counts[i] = counts.get(i, 0) + 1\n",
    "\n",
    "\n",
    "labels = list(counts.keys())\n",
    "data = list(counts.values())\n",
    "colors = sns.color_palette('pastel')[0:5]\n",
    "plt.pie(data, labels = labels, colors = colors, autopct='%.0f%%')\n",
    "plt.show()\n",
    "sns.barplot(x=labels,y=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path_to_file = Path() / \"saved_tweets.csv\"\n",
    "\n",
    "twitter_data = pd.read_csv(path_to_file)\n",
    "tweets = twitter_data[[\"text\"]]\n",
    "\n",
    "\n",
    "tweets = twitter_data[\"text\"].to_list()\n",
    "\n",
    "document = \"\"\n",
    "for tweet in tweets:\n",
    "    document += tweet\n",
    "\n",
    "\n",
    "document\n",
    "\n",
    "\n",
    "\n",
    "tweet_tokens = nltk.word_tokenize(document)\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "tweet_tokens\n",
    "\n",
    "\n",
    "filtered_tweet_text = [w for w in tweet_tokens if w not in stopwords]\n",
    "\n",
    "wordcloud = WordCloud()\n",
    "\n",
    "tweet_freq_dist = nltk.FreqDist(filtered_tweet_text)\n",
    "\n",
    "sorted(tweet_freq_dist,key=tweet_freq_dist.__getitem__,reverse=True)\n",
    "\n",
    "large_words = dict([(k,v) for (k,v) in tweet_freq_dist.items() if len(k) > 3])\n",
    "\n",
    "frequency_dist = nltk.FreqDist(large_words)\n",
    "frequency_dist.plot(30,cumulative=False)\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100,background_color=\"black\").generate_from_frequencies(frequency_dist)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = twitter.search(q=top_trend_on_twitter_query, count=100)\n",
    "\n",
    "r = results[\"statuses\"]\n",
    "\n",
    "tweet_locations = [tweet for tweet in r if tweet[\"place\"] is not None]\n",
    "#print(tweet_locations)\n",
    "\n",
    "with open(\"sample.json\", \"w\") as file:\n",
    "    json.dump(r,file)\n",
    "\n",
    "top_trend_on_twitter_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############   GETTING TWITTER COORDINATES\n",
    "\n",
    "\n",
    "\n",
    "# twitter_response  = twitter._request(url=\" https://stream.twitter.com/1.1/statuses/filter.json\",\n",
    "#                                      params={\"track\": [\"BTS\", \"and\" \"JUNGKOOK\"]}, method=\"GET\",\n",
    "#                                      json_encoded=True)\n",
    "\n",
    "\n",
    "# response = api.request(\"statuses/filter\",{\"track\"🙁\"BTS\", \"and\" \"JUNGKOOK\"})\n",
    "\n",
    "# print(twitter_response)\n",
    "###\n",
    "#coordinates = [] \n",
    "#tweets = response.get_iterator() \n",
    "\n",
    "\n",
    "# count = 0\n",
    "# while count < 100:\n",
    "#     tweet = next(tweets)\n",
    "#     if \"place\" in tweet and tweet [\"place\"] != None:\n",
    "#         place = tweet[\"place\"][\"bounding_box\"][\"coordinates\"][0][0]\n",
    "#         coordinates.append(place)\n",
    "#         count += 1\n",
    "#         print(place)\n",
    "\n",
    "\n",
    "\n",
    "# ukmap = gmplot.GoogleMapPlotter(55.3781, 3.4360, 13)\n",
    "\n",
    "\n",
    "# lat = [twitter_util.results]\n",
    "#print(twitter.results[\"coordinates\"])\n",
    "# ukmap.heatmap(lat,lon)\n",
    "\n",
    "# ukmap.draw(\"/users/usama/desktop/map.html\")\n",
    "# webbrowser.open(\"/users/usama/desktop/map.html\")\n",
    "\n",
    "\n",
    "\n",
    "trending_tweets = pd.read_csv(\"saved_tweets.csv\")\n",
    "trending_tweets = trending_tweets.dropna()\n",
    "\n",
    "import gmplot \n",
    "import webbrowser\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# usa_map =  gmplot.GoogleMapPlotter(53.81604806664296, -3.0548307614209813, 18 )\n",
    "\n",
    "# usa_map.draw(\"/users/usama/desktop/usa.html\")\n",
    "# webbrowser.open_new_tab(\"/users/usama/desktop/usa.html\")\n",
    "\n",
    "map = folium.Map(location=[48, -102], zoom_start=4)\n",
    "\n",
    "\n",
    "marker_cluster = MarkerCluster().add_to(map)\n",
    "# print(trending_tweets)\n",
    "locationlist = trending_tweets[[\"lat\", \"long\"]].values.tolist()\n",
    "locationlist = locationlist[1:-1]\n",
    "print(locationlist)\n",
    "for point in range(0,len(locationlist)):\n",
    "    folium.Marker(locationlist[point], popup=trending_tweets[\"location\"].values.tolist()[point],\n",
    "                icon=folium.Icon(color='darkblue', icon_color='white', icon='male', angle=0, prefix='fa')\n",
    "                ).add_to(marker_cluster)\n",
    "\n",
    "\n",
    "print(top_trend_on_twitter_query)\n",
    "map\n",
    "#print(\"long\", trending_tweets[\"long\"])\n",
    "#print(\"lat\",trending_tweets[\"lat\"])\n",
    "\n",
    "####### FAKE TWEETS\n",
    "# BOUNDING BOX SHOWS EXACT LOCATION, WHILE NAME SHOWS WHAATA USER HAVE PUT IN PROFILE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### STREAMING API\n",
    "\n",
    "\n",
    "from twython import TwythonStreamer\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# counter = 0\n",
    "\n",
    "def process_tweet(data):\n",
    "        # with open(\"stream.json\",\"a\") as file:\n",
    "        #     json.dump(data,file)\n",
    "        tweet = {}\n",
    "        # tweet[\"location\"] = data[\"user\"][\"location\"]\n",
    "        #tweet[\"followers_count\"] = data[\"user\"][\"followers_count\"]\n",
    "        tweet[\"text\"] = data[\"text\"]\n",
    "        tweet[\"created_at\"] = data[\"created_at\"]\n",
    "        #tweet[\"client_platform\"] = data[\"source\"]\n",
    "        #tweet[\"hashtags\"] = [hashtag[\"text\"] for hashtag in data[\"entities\"][\"hashtags\"]]\n",
    "        #tweet[\"retweet_count\"] = data[\"retweet_count\"]\n",
    "        tweet[\"favorite_count\"] = data[\"favorite_count\"]\n",
    "        if \"place\" in data and data[\"place\"] != None:\n",
    "            tweet[\"long\"] = data[\"place\"][\"bounding_box\"][\"coordinates\"][0][0][0]\n",
    "            tweet[\"lat\"] = data[\"place\"][\"bounding_box\"][\"coordinates\"][0][0][1]\n",
    "            print(tweet[\"long\"], tweet[\"lat\"], tweet[\"favorite_count\"])\n",
    "        else:\n",
    "            tweet[\"long\"] = None\n",
    "            tweet[\"lat\"] = None\n",
    "        print(tweet)\n",
    "        return tweet\n",
    "\n",
    "\n",
    "def csv_set_header(data):\n",
    "        with open(\"live_saved_tweets.csv\", \"a\", encoding=\"utf-8\",newline=\"\") as file:\n",
    "            header = data\n",
    "            csv_writer = csv.writer(file)\n",
    "            print(\"hello\")\n",
    "            csv_writer.writerow(header)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "class MyStreamer(TwythonStreamer):\n",
    "    end_time = time.time() + 10\n",
    "    \n",
    "    def on_success(self, data):\n",
    "        global counter \n",
    "        if data[\"lang\"] == \"en\":\n",
    "            tweet_data = process_tweet(data)\n",
    "            # with open(\"stream.json\",\"a\") as file:\n",
    "            #     json.dump(data,file)\n",
    "            print(counter)\n",
    "            if tweet_data[\"long\"] is not None and tweet_data[\"lat\"] is not None:\n",
    "                self.save_to_csv(tweet_data)\n",
    "                counter += counter\n",
    "                print(counter)\n",
    "            if counter >= 3:\n",
    "                self.disconnect()\n",
    "\n",
    "\n",
    "    def save_to_csv(self, tweet):\n",
    "        with open(r'live_saved_tweets.csv', \"a\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(list(tweet.values()))\n",
    "\n",
    "    # def on_error(self, status_code, data,**kargs):\n",
    "    #     print(status_code,data)\n",
    "    #     self.disconnect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stream = MyStreamer(credentials[\"API_KEY\"], credentials[\"API_SECRET\"], credentials[\"ACCESS_TOKEN\"], credentials[\"ACCESS_TOKEN_SECRET\"])\n",
    "csv_set_header([\"text\",\"created_at\",\"favorite_count\",\"long\",\"lat\"])\n",
    "\n",
    "#stream.statuses.filter(track=[top_trend_on_twitter_query], location= [ 48,-102], result_type=\"popular\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## STREAMER TREND \n",
    "# top_trend_on_twitter_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# For popular tweets location\n",
    "\n",
    "# params = {}\n",
    "# popular_tweets = Twitter_Utility()\n",
    "# popular_tweets.search_tweet({params[\"q\"]:top_trend_on_twitter_query,\n",
    "#                                 params[\"count\"]:100, \n",
    "#                                 params[\"result_type\"]:\"popular\",\n",
    "#                                 params[\"has:geo\"]:True})\n",
    "# popular_tweets.save_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns_1\n",
    "df_tweets = pd.read_csv(\"saved_tweets.csv\")\n",
    "pattern = r'Android|iPhone|Web App|Tweetdeck|Tweetbot|TweetDeck'\n",
    "#df_tweets = df_tweets[df_tweets[\"client_platform\"].isin(tweet_devices)]\n",
    "# df_tweets['client_platform'] =  [re.search(r'iPhone|Android|Web App','', str(x)) for x in df_tweets['client_platform']]\n",
    "filter=  df_tweets['client_platform'].str.contains(pattern)\n",
    "df_tweets = df_tweets[filter]\n",
    "\n",
    "# print(df_tweets.head())\n",
    "df_tweets[\"clients_platform\"] = twitter_client_system\n",
    "\n",
    "\n",
    "#print(df_tweets.tail())\n",
    "\n",
    "ax2 = sns_1.boxplot(x=\"clients_platform\", y=\"retweet_count\" ,data=df_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"retweet_count\", y=\"favorite_count\",\n",
    "             data=df_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"clients_platform\", y=\"favorite_count\" ,data=df_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_tweets[\"retweet_count\"],kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"retweet_count\",y=\"favorite_count\" ,data=df_tweets, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.rugplot(df_tweets[\"favorite_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.rugplot(df_tweets[\"followers_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"followers_count\", y=\"favorite_count\", data=df_tweets)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2edc8d72492fce6da8c9f093076faead55b47d1ae243b1a3a4b5a0eb61a5ea42"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
