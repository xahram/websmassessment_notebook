{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "import json\n",
    "import csv\n",
    "\n",
    "credentials = {}\n",
    "credentials[\"API_KEY\"] = \"kfMHh8BuUXMCQ9S57eHlZPtp3\"\n",
    "credentials[\"API_SECRET\"] = \"YBBa1e6czkDK30KOpdMPN20QCWao2M8ONvCodfnH8gWW1AOXIu\"\n",
    "credentials[\"ACCESS_TOKEN\"] = \"1501925600105218050-lOCbvbbF4Ikoy9ublZQi4NAiGsAN84\"\n",
    "credentials[\"ACCESS_TOKEN_SECRET\"] = \"cS5qbsBa9IvF6ORoKs6UZtGz5KbLe0nY9lINwWGeK35ao\"\n",
    "\n",
    "\n",
    "with open(\"credentials.json\", \"w\") as file:\n",
    "    json.dump(credentials, file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTHENTICATION OF TWITTER TOKENS OAUTH\n",
    "from twython import Twython\n",
    "import json\n",
    "\n",
    "credentials = {}\n",
    "with open(\"credentials.json\", \"r\") as file:\n",
    "    credentials = json.load(file)\n",
    "\n",
    "twitter = Twython(app_key=credentials[\"API_KEY\"],\n",
    "                 app_secret=credentials[\"API_SECRET\"], \n",
    "                 oauth_token=credentials[\"ACCESS_TOKEN\"],\n",
    "                 oauth_token_secret=credentials[\"ACCESS_TOKEN_SECRET\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Twitter_Utility:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "\n",
    "    def process_tweet(self, data):\n",
    "        tweet = {}\n",
    "        tweet[\"location\"] = data[\"user\"][\"location\"]\n",
    "        tweet[\"followers_count\"] = data[\"user\"][\"followers_count\"]\n",
    "        tweet[\"text\"] = data[\"text\"]\n",
    "        tweet[\"client_platform\"] = data[\"source\"]\n",
    "        tweet[\"hashtags\"] = [hashtag[\"text\"] for hashtag in data[\"entities\"][\"hashtags\"]]\n",
    "        tweet[\"retweet_count\"] = data[\"retweet_count\"]\n",
    "        tweet[\"favorited\"] = data[\"favorited\"]\n",
    "        tweet[\"coordinates\"] = data[\"coordinates\"]\n",
    "\n",
    "        return tweet\n",
    "\n",
    "\n",
    "    def csv_set_header(self, tweet):\n",
    "         with open(\"saved_tweets.csv\", \"a\", encoding=\"utf-8\",newline=\"\") as file:\n",
    "            header = [key for key in self.process_tweet(tweet)]\n",
    "            csv_writer = csv.writer(file)\n",
    "            csv_writer.writerow(header)\n",
    "        \n",
    "    def save_to_csv(self, tweet):\n",
    "        with open(\"saved_tweets.csv\", \"a\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "            csv_writer = csv.writer(file)\n",
    "            tweet_content = [tweet[key] for key in tweet]\n",
    "            csv_writer.writerow(tweet_content)\n",
    "            return tweet_content\n",
    "\n",
    "    def search_tweet(self, search_query,count,lang=\"en\",result_type=\"mixed\"):\n",
    "        results = twitter.search(q=search_query, count= count,lang=lang, result_type=result_type)   \n",
    "        self.results = results\n",
    "        return results\n",
    "        \n",
    "    \n",
    "    def save_tweets(self):\n",
    "        with open(\"someran.json\", \"a\") as file:\n",
    "            json.dump(self.results[\"statuses\"][0], file)\n",
    "        if self.results.get(\"statuses\"):\n",
    "                self.csv_set_header(self.results[\"statuses\"][0])\n",
    "                for result in self.results[\"statuses\"]:\n",
    "                    tweet = self.process_tweet(result) \n",
    "                    self.save_to_csv(tweet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "twitter_util = Twitter_Utility()\n",
    "#twitter_util.search_tweet(\"Ukraine\", count=100)\n",
    "#twitter_util.save_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TWITTER WORLDWIDE TRENDS\n",
    "import pandas as pd\n",
    "\n",
    "def twitter_trends_url(url, id):\n",
    "    url = twitter.construct_api_url(api_url=url, id = id)\n",
    "    print(url)\n",
    "    return url\n",
    "\n",
    "response = twitter._request(url= twitter_trends_url(\"https://api.twitter.com/1.1/trends/place.json\",id=23424975),\n",
    "                    json_encoded=True,\n",
    "                    method=\"GET\")\n",
    "\n",
    "with open(\"worldtrends.json\", \"w\") as file:\n",
    "        json.dump(response, file)\n",
    "\n",
    "\n",
    "top_15_trends_list = list(response[0][\"trends\"])[0:15]\n",
    "\n",
    "top_15_trends_list\n",
    "df = pd.DataFrame(columns=[\"name\",\"tweet_volume\",\"url\",\"query\"])\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/10715965/create-a-pandas-dataframe-by-appending-one-row-at-a-time\n",
    "\n",
    "for index in range(len(top_15_trends_list)):\n",
    "    df.loc[index] = [top_15_trends_list[index][\"name\"], \n",
    "                        top_15_trends_list[index][\"tweet_volume\"], \n",
    "                        top_15_trends_list[index][\"url\"],\n",
    "                        top_15_trends_list[index][\"query\"]\n",
    "                        ] \n",
    "\n",
    "\n",
    "\n",
    "df\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### BARPLOT FOR WORLDWIDE TRENDS\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# type(df[\"tweet_volume\"])\n",
    "df = df.dropna()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sorted_df = df.sort_values(['tweet_volume'], ascending=False).reset_index(drop=True)\n",
    "sns.barplot(data=sorted_df,\n",
    "             x=\"name\", \n",
    "             y=\"tweet_volume\",\n",
    "             order=sorted_df.name)\n",
    "plt.xlabel(\"Worldwide Twitter Trends\", size= 15)             \n",
    "plt.ylabel(\"Tweet Count\", size= 15)             \n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trend_url = df[df[\"name\"]==\"Ted Cruz\"].url.to_string(index=False)\n",
    "#trend_tweets = twitter_util.search_tweet(sorted_df.iat[0,0], count=10)\n",
    "\n",
    "trend_url = df[df[\"name\"]==\"Ted Cruz\"].url.to_string(index=False)\n",
    "# trend_url\n",
    "\n",
    "top_trend_on_twitter_query = sorted_df.iat[0,3]\n",
    "trend_tweets = twitter_util.search_tweet(top_trend_on_twitter_query,count=100)\n",
    "twitter_util.results = trend_tweets\n",
    "twitter_util.save_tweets()\n",
    "\n",
    "with open(\"ran.json\", \"a\") as file:\n",
    "    json.dump(trend_tweets,file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# PIECHART FOR DEVICES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.font_manager\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['text.color'] = '#909090'\n",
    "plt.rcParams['axes.labelcolor']= '#909090'\n",
    "plt.rcParams['xtick.color'] = '#909090'\n",
    "plt.rcParams['ytick.color'] = '#909090'\n",
    "plt.rcParams['font.size']=12\n",
    "\n",
    "\n",
    "# labels = [w for w,k in itertools.groupby(twitter_client_system , lambda x : x)]\n",
    "from pathlib import Path\n",
    "import re\n",
    "tweets_trending_file_path = Path() / \"saved_tweets.csv\"\n",
    "trending_tweets = pd.read_csv(tweets_trending_file_path)\n",
    "\n",
    "\n",
    "import re\n",
    "client_system = trending_tweets[\"client_platform\"].tolist()\n",
    "\n",
    "#print(client_system)\n",
    "twitter_client_system = []\n",
    "for i in client_system:\n",
    "    match = re.search(r'Android|iPhone|Web App',i)\n",
    "    if match is not None:\n",
    "      device = match.group()\n",
    "      twitter_client_system.append(device)\n",
    "\n",
    "print(twitter_client_system) \n",
    "\n",
    "\n",
    "counts = dict()\n",
    "for i in twitter_client_system:\n",
    "  counts[i] = counts.get(i, 0) + 1\n",
    "\n",
    "\n",
    "labels = list(counts.keys())\n",
    "data = list(counts.values())\n",
    "colors = sns.color_palette('pastel')[0:5]\n",
    "plt.pie(data, labels = labels, colors = colors, autopct='%.0f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path_to_file = Path() / \"saved_tweets.csv\"\n",
    "\n",
    "twitter_data = pd.read_csv(path_to_file)\n",
    "tweets = twitter_data[[\"text\"]]\n",
    "\n",
    "\n",
    "tweets = twitter_data[\"text\"].to_list()\n",
    "\n",
    "document = \"\"\n",
    "for tweet in tweets:\n",
    "    document += tweet\n",
    "\n",
    "\n",
    "document\n",
    "\n",
    "\n",
    "\n",
    "tweet_tokens = nltk.word_tokenize(document)\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "tweet_tokens\n",
    "\n",
    "\n",
    "filtered_tweet_text = [w for w in tweet_tokens if w not in stopwords]\n",
    "\n",
    "wordcloud = WordCloud()\n",
    "\n",
    "tweet_freq_dist = nltk.FreqDist(filtered_tweet_text)\n",
    "\n",
    "sorted(tweet_freq_dist,key=tweet_freq_dist.__getitem__,reverse=True)\n",
    "\n",
    "large_words = dict([(k,v) for (k,v) in tweet_freq_dist.items() if len(k) > 3])\n",
    "\n",
    "frequency_dist = nltk.FreqDist(large_words)\n",
    "frequency_dist.plot(30,cumulative=False)\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100,background_color=\"black\").generate_from_frequencies(frequency_dist)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = twitter.search(q=top_trend_on_twitter_query, count=100)\n",
    "\n",
    "r = results[\"statuses\"]\n",
    "\n",
    "tweet_locations = [tweet for tweet in r if tweet[\"place\"] is not None]\n",
    "print(tweet_locations)\n",
    "\n",
    "with open(\"sample.json\", \"w\") as file:\n",
    "    json.dump(r,file)\n",
    "\n",
    "top_trend_on_twitter_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmplot \n",
    "import webbrowser\n",
    "\n",
    "# ukmap = gmplot.GoogleMapPlotter(55.3781, 3.4360, 13)\n",
    "\n",
    "\n",
    "# lat = [twitter_util.results]\n",
    "print(twitter.results[\"coordinates\"])\n",
    "# ukmap.heatmap(lat,lon)\n",
    "\n",
    "# ukmap.draw(\"/users/usama/desktop/map.html\")\n",
    "# webbrowser.open(\"/users/usama/desktop/map.html\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2edc8d72492fce6da8c9f093076faead55b47d1ae243b1a3a4b5a0eb61a5ea42"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
