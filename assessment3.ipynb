{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### GET API KEYS\n",
    "from twython import Twython\n",
    "import json\n",
    "import csv\n",
    "\n",
    "credentials = {}\n",
    "credentials[\"API_KEY\"] = \"AZVjw2iplhA5dNszUeGxjOgjv\"\n",
    "credentials[\"API_SECRET\"] = \"RZ8E6R59gC9oW92Qz4YFRgygACvD4onQRzqyUTPRzKfnnQSW3H\"\n",
    "credentials[\"ACCESS_TOKEN\"] = \"1501925600105218050-7xtRvXQGeA6wkft2kPoA5JOj0PnBmq\"\n",
    "credentials[\"ACCESS_TOKEN_SECRET\"] = \"0Fl3hzhnogVszJGEorAh9r7XBRspU7XQaWERfC8JXtA2M\"\n",
    "\n",
    "\n",
    "##### SAVE CREDENTIALS IN JSON FILE\n",
    "with open(\"credentials.json\", \"w\") as file:\n",
    "    json.dump(credentials, file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTHENTICATION OF TWITTER TOKENS OAUTH\n",
    "from twython import Twython\n",
    "import json\n",
    "\n",
    "\n",
    "########## GET TOKENS FROM THE SAVED JSON FILE\n",
    "credentials = {}\n",
    "with open(\"credentials.json\", \"r\") as file:\n",
    "    credentials = json.load(file)\n",
    "\n",
    "twitter = Twython(app_key=credentials[\"API_KEY\"],\n",
    "                 app_secret=credentials[\"API_SECRET\"], \n",
    "                 oauth_token=credentials[\"ACCESS_TOKEN\"],\n",
    "                 oauth_token_secret=credentials[\"ACCESS_TOKEN_SECRET\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ UTILITY CLASS TO PROCESS TWEETS SAVING\n",
    "\n",
    "class Twitter_Utility:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.results = {\"statuses\":[]}\n",
    "        self.header_counter = 0\n",
    "    \n",
    "    ############### EXTRACT INDIVIDUAL TWEET ATTRIBUTES\n",
    "    def process_tweet(self, data):\n",
    "        tweet = {}\n",
    "        tweet[\"created_at\"] = data[\"created_at\"]\n",
    "        tweet[\"location\"] = data[\"user\"][\"location\"]\n",
    "        tweet[\"followers_count\"] = data[\"user\"][\"followers_count\"]\n",
    "        tweet[\"verified\"] = data[\"user\"][\"verified\"]\n",
    "        tweet[\"text\"] = data[\"text\"]\n",
    "        tweet[\"client_platform\"] = data[\"source\"]\n",
    "        tweet[\"hashtags\"] = [hashtag[\"text\"] for hashtag in data[\"entities\"][\"hashtags\"]]\n",
    "        tweet[\"retweet_count\"] = data[\"retweet_count\"]\n",
    "        # tweet[\"favorited\"] = data[\"favorited\"]\n",
    "        tweet[\"favorite_count\"] = data[\"favorite_count\"]\n",
    "        if \"place\" in data and data[\"place\"] != None:\n",
    "            tweet[\"long\"] = data[\"place\"][\"bounding_box\"][\"coordinates\"][0][0][0]\n",
    "            tweet[\"lat\"] = data[\"place\"][\"bounding_box\"][\"coordinates\"][0][0][1]\n",
    "        else:          \n",
    "            tweet[\"long\"] = None\n",
    "            tweet[\"lat\"] = None\n",
    "\n",
    "        return tweet\n",
    "\n",
    "\n",
    "    ######### SAVE THE CSV COLUMN NAMES FROM THE PROCESSED TWEETS KEYS\n",
    "    def csv_set_header(self, tweet):\n",
    "        if self.header_counter == 0:\n",
    "          with open(\"saved_tweets.csv\", \"a\", encoding=\"utf-8\",newline=\"\") as file:\n",
    "            header = [key for key in self.process_tweet(tweet)]\n",
    "            csv_writer = csv.writer(file)\n",
    "            csv_writer.writerow(header)\n",
    "            self.header_counter = 1\n",
    "        \n",
    "    ######### FUNCTIN TO SAVE TWEEETS\n",
    "    def save_to_csv(self, tweet):\n",
    "            with open(\"saved_tweets.csv\", \"a\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "                csv_writer = csv.writer(file)\n",
    "                tweet_content = [tweet[key] for key in tweet]\n",
    "                csv_writer.writerow(tweet_content)\n",
    "                return tweet_content\n",
    "\n",
    "    ########### FUNCTION TO SEARCH TWEETS BY USING GIVEN  QUERY\n",
    "    def search_tweet(self, search_query,count,lang=\"en\",result_type=\"mixed\"):\n",
    "        for i in range(49):\n",
    "            results = twitter.search(q=search_query, count= count,lang=lang, result_type=result_type)   \n",
    "            self.results[\"statuses\"].extend(results[\"statuses\"])\n",
    "        # self.results[\"statuses\"] = self.results[\"statuses\"].append(results[\"statuses\"])\n",
    "        # self.results[\"statuses\"] = self.results[\"statuses\"][0]\n",
    "        # self.results[\"statuses\"] = self.results[\"statuses\"][0]\n",
    "\n",
    "        \n",
    "        #print(\"multi list\", self.results)\n",
    "        return self.results\n",
    "        \n",
    "    #### FUNCTION TO PROCESS WHOLE FETCHING & SAVING OF TWEETS \n",
    "    def save_tweets(self):\n",
    "        if self.results.get(\"statuses\"):\n",
    "                #self.csv_set_header(self.results[\"statuses\"][0])\n",
    "                for result in self.results[\"statuses\"]:\n",
    "                    tweet = self.process_tweet(result) \n",
    "                    self.save_to_csv(tweet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "twitter_util = Twitter_Utility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TWITTER USA TRENDS FETCHING\n",
    "import pandas as pd\n",
    "\n",
    "######### UTILITY FUNCTION TO CONSTRUCT THE API URL OF TRENDS FOR A PARTICULAR LOCATTION\n",
    "def twitter_trends_url(url, id):\n",
    "    url = twitter.construct_api_url(api_url=url, id = id)\n",
    "    print(url)\n",
    "    return url\n",
    "\n",
    "\n",
    "\n",
    "######################### WOEIDS OF DIFFERENT LOCATIONS\n",
    "# WOEID \n",
    "# UK=23424975 \n",
    "# USA=23424977\n",
    "# WORLDWIDE=1\n",
    "# Birmingham=12723\n",
    "\n",
    "\n",
    "response = twitter._request(url= twitter_trends_url(\"https://api.twitter.com/1.1/trends/place.json\",id=23424977),\n",
    "                    json_encoded=True,\n",
    "                    method=\"GET\")\n",
    "\n",
    "\n",
    "############# SAVE TRENDS INTO JSON\n",
    "with open(\"worldtrends.json\", \"w\") as file:\n",
    "        json.dump(response, file)\n",
    "\n",
    "\n",
    "top_15_trends_list = list(response[0][\"trends\"])[0:15]\n",
    "\n",
    "top_15_trends_list\n",
    "\n",
    "#################### CREATE DATAFRAME COLUMNS FOR TRENDS\n",
    "df = pd.DataFrame(columns=[\"name\",\"tweet_volume\",\"url\",\"query\"])\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/10715965/create-a-pandas-dataframe-by-appending-one-row-at-a-time\n",
    "\n",
    "\n",
    "############ FOR EVERY TREND SAVE ITS CONTENT INTO RESPECTIVE LOCATION\n",
    "for index in range(len(top_15_trends_list)):\n",
    "    df.loc[index] = [top_15_trends_list[index][\"name\"], \n",
    "                        top_15_trends_list[index][\"tweet_volume\"], \n",
    "                        top_15_trends_list[index][\"url\"],\n",
    "                        top_15_trends_list[index][\"query\"]\n",
    "                        ] \n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### BARPLOT FOR USA TRENDS\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "############ DROP ANY COLUMNS THAT DON\"T HAVE ANY TWEET VOLUME OR TWEETS NUMBERS\n",
    "df = df.dropna()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "##### SORT THE DATAFRAME TO SHOW TOP TRENDS FROM TOP TO BOTTOM\n",
    "sorted_df = df.sort_values(['tweet_volume'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "###### PLOT THE BARPLOT OF TRENDING TOPICS\n",
    "sns.barplot(data=sorted_df,\n",
    "             x=\"name\", \n",
    "             y=\"tweet_volume\",\n",
    "             order=sorted_df.name)\n",
    "plt.xlabel(\"USA Twitter Trends\", size= 15)             \n",
    "plt.ylabel(\"Tweet Count\", size= 15)             \n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_trend_on_twitter_query = sorted_df.iat[0,0]\n",
    "trend_tweets = twitter_util.search_tweet(top_trend_on_twitter_query,count=100)\n",
    "\n",
    "\n",
    "# SHOW THE QUERY OF TOP TREND USED TO FETCH TWEETS\n",
    "\n",
    "print(top_trend_on_twitter_query)\n",
    "twitter_util.save_tweets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# PIECHART FOR DEVICES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.font_manager\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['text.color'] = '#909090'\n",
    "plt.rcParams['axes.labelcolor']= '#909090'\n",
    "plt.rcParams['xtick.color'] = '#909090'\n",
    "plt.rcParams['ytick.color'] = '#909090'\n",
    "plt.rcParams['font.size']=12\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "tweets_trending_file_path = Path() / \"saved_tweets.csv\"\n",
    "trending_tweets = pd.read_csv(tweets_trending_file_path)\n",
    "\n",
    "\n",
    "import re\n",
    "client_system = trending_tweets[\"client_platform\"].tolist()\n",
    "\n",
    "\n",
    "# REGULAR EXPRESSION IS USED ON EVERY DATA POINT IN DF TO GET THE CLIENT PLATFORM OUT OF THE \n",
    "# HTML ANCHOR TAG USED BY TWITTER FOR SHOWING CLIENT DEVICE\n",
    "\n",
    "twitter_client_system = []\n",
    "for i in client_system:\n",
    "    match = re.search(r'Android|iPhone|Web App|Tweetdeck|TweetDeck|Tweetbot',i)\n",
    "    if match is not None:\n",
    "      device = match.group()\n",
    "      twitter_client_system.append(device)\n",
    "\n",
    "counts = dict()\n",
    "for i in twitter_client_system:\n",
    "  counts[i] = counts.get(i, 0) + 1\n",
    "\n",
    "\n",
    "labels = list(counts.keys())\n",
    "data = list(counts.values())\n",
    "colors = sns.color_palette('pastel')[0:5]\n",
    "plt.pie(data, labels = labels, colors = colors, autopct='%.0f%%')\n",
    "plt.title(\"Device percentage used in trend\", size= 10)        \n",
    "plt.legend(labels)                   \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ DEVICES BARPLOT\n",
    "\n",
    "sns.barplot(x=labels,y=data)\n",
    "plt.xlabel(\"Client Platform\", size=10)\n",
    "plt.ylabel(\"Number of devices\", size=10)\n",
    "plt.title(\"Device percentage used in trend\", size= 15)                   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# PIECHART FOR DEVICES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.font_manager\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['text.color'] = '#909090'\n",
    "plt.rcParams['axes.labelcolor']= '#909090'\n",
    "plt.rcParams['xtick.color'] = '#909090'\n",
    "plt.rcParams['ytick.color'] = '#909090'\n",
    "plt.rcParams['font.size']=12\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "tweets_trending_file_path = Path() / \"saved_tweets.csv\"\n",
    "trending_tweets = pd.read_csv(tweets_trending_file_path)\n",
    "\n",
    "\n",
    "import re\n",
    "verified_user_tweets = trending_tweets[\"verified\"].tolist()\n",
    "\n",
    "counts = dict()\n",
    "for i in verified_user_tweets:\n",
    "  counts[i] = counts.get(i, 0) + 1\n",
    "\n",
    "\n",
    "labels = list(counts.keys())\n",
    "legends = []\n",
    "for label in labels:\n",
    "    if label == True:\n",
    "        legends.append(\"Verified\")\n",
    "    else:\n",
    "        legends.append(\"Not Verified\")\n",
    "data = list(counts.values())\n",
    "colors = sns.color_palette('pastel')[0:5]\n",
    "plt.pie(data, labels = legends, colors = colors, autopct='%1.0f%%', pctdistance=1.1, labeldistance=1.2)\n",
    "plt.title(\"Device percentage used in trend\", size= 10)   \n",
    "plt.legend(labels=legends)                    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path_to_file = Path() / \"saved_tweets.csv\"\n",
    "\n",
    "twitter_data = pd.read_csv(path_to_file)\n",
    "tweets = twitter_data[[\"text\"]]\n",
    "\n",
    "\n",
    "tweets = twitter_data[\"text\"].to_list()\n",
    "\n",
    "document = \"\"\n",
    "for tweet in tweets:\n",
    "    document += tweet\n",
    "\n",
    "\n",
    "document\n",
    "\n",
    "\n",
    "\n",
    "tweet_tokens = nltk.word_tokenize(document)\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "tweet_tokens\n",
    "\n",
    "\n",
    "filtered_tweet_text = [w for w in tweet_tokens if w not in stopwords]\n",
    "\n",
    "wordcloud = WordCloud()\n",
    "\n",
    "tweet_freq_dist = nltk.FreqDist(filtered_tweet_text)\n",
    "\n",
    "sorted(tweet_freq_dist,key=tweet_freq_dist.__getitem__,reverse=True)\n",
    "\n",
    "large_words = dict([(k,v) for (k,v) in tweet_freq_dist.items() if len(k) > 3])\n",
    "\n",
    "frequency_dist = nltk.FreqDist(large_words)\n",
    "frequency_dist.plot(30,cumulative=False)\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100,background_color=\"black\").generate_from_frequencies(frequency_dist)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = twitter.search(q=top_trend_on_twitter_query, count=100)\n",
    "\n",
    "r = results[\"statuses\"]\n",
    "\n",
    "tweet_locations = [tweet for tweet in r if tweet[\"place\"] is not None]\n",
    "\n",
    "with open(\"sample.json\", \"w\") as file:\n",
    "    json.dump(r,file)\n",
    "\n",
    "top_trend_on_twitter_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### STREAMING API\n",
    "\n",
    "############### ALTHOUGH STREAMING WAS USED FOR TESTING LOCATION TWEETS \n",
    "#### BUT THE TWEETS FROM STREAMS AREN\"T USED THIS IS HERE JUST FOR REFERENCE PURPOSES \n",
    "\n",
    "from twython import TwythonStreamer\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# counter = 0\n",
    "\n",
    "def process_tweet(data):\n",
    "        \n",
    "        tweet = {}\n",
    "        tweet[\"text\"] = data[\"text\"]\n",
    "        tweet[\"created_at\"] = data[\"created_at\"]\n",
    "        tweet[\"favorite_count\"] = data[\"favorite_count\"]\n",
    "        if \"place\" in data and data[\"place\"] != None:\n",
    "            tweet[\"long\"] = data[\"place\"][\"bounding_box\"][\"coordinates\"][0][0][0]\n",
    "            tweet[\"lat\"] = data[\"place\"][\"bounding_box\"][\"coordinates\"][0][0][1]\n",
    "            print(tweet[\"long\"], tweet[\"lat\"], tweet[\"favorite_count\"])\n",
    "        else:\n",
    "            tweet[\"long\"] = None\n",
    "            tweet[\"lat\"] = None\n",
    "        print(tweet)\n",
    "        return tweet\n",
    "\n",
    "\n",
    "def csv_set_header(data):\n",
    "        with open(\"live_saved_tweets.csv\", \"a\", encoding=\"utf-8\",newline=\"\") as file:\n",
    "            header = data\n",
    "            csv_writer = csv.writer(file)\n",
    "            print(\"hello\")\n",
    "            csv_writer.writerow(header)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "class MyStreamer(TwythonStreamer):\n",
    "    end_time = time.time() + 10\n",
    "    \n",
    "    def on_success(self, data):\n",
    "        global counter \n",
    "        if data[\"lang\"] == \"en\":\n",
    "            tweet_data = process_tweet(data)\n",
    "            print(counter)\n",
    "            \n",
    "            \n",
    "            if tweet_data[\"long\"] is not None and tweet_data[\"lat\"] is not None:\n",
    "                self.save_to_csv(tweet_data)\n",
    "                counter += counter\n",
    "                print(counter)\n",
    "            if counter >= 3:\n",
    "                self.disconnect()\n",
    "\n",
    "\n",
    "    def save_to_csv(self, tweet):\n",
    "        with open(r'live_saved_tweets.csv', \"a\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(list(tweet.values()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stream = MyStreamer(credentials[\"API_KEY\"], credentials[\"API_SECRET\"], credentials[\"ACCESS_TOKEN\"], credentials[\"ACCESS_TOKEN_SECRET\"])\n",
    "csv_set_header([\"text\",\"created_at\",\"favorite_count\",\"long\",\"lat\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############   GETTING TWITTER COORDINATES\n",
    "\n",
    "trending_tweets = pd.read_csv(\"saved_tweets.csv\")\n",
    "trending_tweets = trending_tweets.dropna()\n",
    "\n",
    "import gmplot \n",
    "import webbrowser\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "\n",
    "########### UNCOMMENT BELOW CODE TO PLOT MAP USING GMPLOT\n",
    "# usa_map =  gmplot.GoogleMapPlotter(53.81604806664296, -3.0548307614209813, 18 )\n",
    "# usa_map.draw(\"/users/usama/desktop/usa.html\")\n",
    "# webbrowser.open_new_tab(\"/users/usama/desktop/usa.html\")\n",
    "\n",
    "map = folium.Map(location=[48, -102], zoom_start=4)\n",
    "\n",
    "\n",
    "marker_cluster = MarkerCluster().add_to(map)\n",
    "locationlist = trending_tweets[[\"lat\", \"long\"]].values.tolist()\n",
    "locationlist = locationlist[1:-1]\n",
    "print(locationlist)\n",
    "for point in range(0,len(locationlist)):\n",
    "    folium.Marker(locationlist[point], popup=trending_tweets[\"location\"].values.tolist()[point],\n",
    "                icon=folium.Icon(color='darkblue', icon_color='white', icon='male', angle=0, prefix='fa')\n",
    "                ).add_to(marker_cluster)\n",
    "\n",
    "\n",
    "#print(top_trend_on_twitter_query)\n",
    "map\n",
    "\n",
    "####### FAKE TWEETS\n",
    "# BOUNDING BOX SHOWS EXACT LOCATION, WHILE NAME SHOWS WHAATA USER HAVE PUT IN PROFILE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## STREAMER TREND \n",
    "# top_trend_on_twitter_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# For popular tweets location\n",
    "\n",
    "# params = {}\n",
    "# popular_tweets = Twitter_Utility()\n",
    "# popular_tweets.search_tweet({params[\"q\"]:top_trend_on_twitter_query,\n",
    "#                                 params[\"count\"]:100, \n",
    "#                                 params[\"result_type\"]:\"popular\",\n",
    "#                                 params[\"has:geo\"]:True})\n",
    "# popular_tweets.save_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############### BOX PLOT FOR DEVICES & TWEETS\n",
    "import seaborn as sns_1\n",
    "df_tweets = pd.read_csv(\"saved_tweets.csv\")\n",
    "pattern = r'Android|iPhone|Web App|Tweetdeck|TweetDeck|Tweetbot'\n",
    "\n",
    "filter=  df_tweets['client_platform'].str.contains(pattern)\n",
    "df_tweets = df_tweets[filter]\n",
    "\n",
    "df_tweets[\"clients_platform\"] = twitter_client_system\n",
    "\n",
    "ax2 = sns_1.boxplot(x=\"clients_platform\", y=\"retweet_count\" ,data=df_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ RELAION BETWEETN RETWEEET VS LIKES\n",
    "\n",
    "sns.relplot(x=\"retweet_count\", y=\"favorite_count\",\n",
    "             data=df_tweets)\n",
    "plt.xlabel(\"Number of Retweets\", size=10)\n",
    "plt.ylabel(\"Number of Likes\", size=10)\n",
    "plt.title(\"Relation of Retweets Vs Likes\", size= 15)                       \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"clients_platform\", y=\"favorite_count\" ,data=df_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_tweets[\"retweet_count\"],kde=True)\n",
    "\n",
    "plt.xlabel(\"Number of Retweets\", size=10)\n",
    "plt.ylabel(\"Retweet Count\", size=10)\n",
    "plt.title(\"Distribution Of Retweet Count\", size= 15)                       \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"retweet_count\",y=\"favorite_count\" ,data=df_tweets, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.rugplot(df_tweets[\"favorite_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.rugplot(df_tweets[\"followers_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"followers_count\", y=\"favorite_count\", data=df_tweets)\n",
    "\n",
    "plt.xlabel(\"Follower Count\", size=10)\n",
    "plt.ylabel(\"Number of Likes\", size=10)\n",
    "plt.title(\"Relation of Followers Vs Likes\", size= 15)                       \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################  ASESSMENT 3 ###############################\n",
    "\n",
    "top_trend_on_twitter_query = \"Liverpool\"\n",
    " \n",
    "from pathlib import Path\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk \n",
    "# from sklearn.feature_extraction.text import CountVectorize\n",
    "\n",
    "############### USE THIS FOR FIRST TIME TO DOWNLOAD VADER LEXICON\n",
    "#import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "\n",
    "\n",
    "path_to_tweets = Path() / \"saved_tweets.csv\"\n",
    "tweets = pd.read_csv(path_to_tweets)\n",
    "\n",
    "\n",
    "\n",
    "tweets_ = tweets[\"text\"].to_list()\n",
    "no_of_tweets = len(tweets_)\n",
    "\n",
    "\n",
    "for tweet in tweets_:\n",
    "    tweet_list.append(tweet)\n",
    "    analysis = TextBlob(tweet)\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(tweet)\n",
    "    neg = score[\"neg\"]\n",
    "    neu = score[\"neu\"]\n",
    "    pos = score[\"pos\"]\n",
    "    comp = score[\"compound\"]\n",
    "    polarity += analysis.sentiment.polarity\n",
    "    if neg > pos:\n",
    "        negative_list.append(tweet)\n",
    "        negative += 1\n",
    "\n",
    "    elif pos > neg:\n",
    "         positive_list.append(tweet)\n",
    "         positive += 1  \n",
    "\n",
    "    elif pos == neg:\n",
    "        neutral_list.append(tweet)\n",
    "        neutral += 1  \n",
    "\n",
    "\n",
    "\n",
    "def percentage(part,whole):\n",
    " return 100 * float(part)/float(whole)\n",
    "\n",
    "positive = percentage(positive, no_of_tweets)\n",
    "negative = percentage(negative, no_of_tweets)\n",
    "neutral = percentage(neutral, no_of_tweets)\n",
    "polarity = percentage(polarity, no_of_tweets)\n",
    "    \n",
    "\n",
    "\n",
    "positive = format(positive, \".1f\")\n",
    "negative = format(negative, \".1f\")\n",
    "neutral = format(neutral, \".1f\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### WORDCLOUD UTILITY FUNCTION\n",
    "import re\n",
    "\n",
    "\n",
    "def wordcloud_generator(tweet_list, title, stopwords):\n",
    "    document = \"\"\n",
    "    for tweet in tweet_list:\n",
    "        document += tweet\n",
    "\n",
    "\n",
    "    document = re.sub(\"https\",\" \", document)\n",
    "    tweet_tokens = nltk.word_tokenize(document)\n",
    "\n",
    "    stopwords = set(stopwords)\n",
    "\n",
    "    filtered_tweet_text = [w for w in tweet_tokens if w not in stopwords]\n",
    "    wordcloud = WordCloud()\n",
    "\n",
    "    tweet_freq_dist = nltk.FreqDist(filtered_tweet_text)\n",
    "\n",
    "    sorted(tweet_freq_dist,key=tweet_freq_dist.__getitem__,reverse=True)\n",
    "\n",
    "    large_words = dict([(k,v) for (k,v) in tweet_freq_dist.items() if len(k) > 3])\n",
    "\n",
    "    frequency_dist = nltk.FreqDist(large_words)\n",
    "    frequency_dist.plot(30,cumulative=False)\n",
    "\n",
    "    wordcloud = WordCloud(max_font_size=50, max_words=100,background_color=\"black\").generate_from_frequencies(frequency_dist)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.title(title, size= 15)  \n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GENERATE WORCLOUD FOR TWEETS\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "custom_stop_words = [\"this\",\"they\",\"This\",\"That\",\"They\",\"RT\",\"rt\"] \n",
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "for i in custom_stop_words:\n",
    "    stopwords.append(i)\n",
    "\n",
    "wordcloud_generator(positive_list, \"Positive Sentiment Tweets Wordcloud\", stopwords)\n",
    "wordcloud_generator(negative_list, \"Negative Sentiment Tweets Wordcloud\", stopwords)\n",
    "wordcloud_generator(neutral_list, \"Neutral Tweets Wordcloud\",stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# tweet_list = pd.DataFrame(tweet_list)\n",
    "# neutral_list = pd.DataFrame(neutral_list)\n",
    "# negative_list = pd.DataFrame(negative_list)\n",
    "# positive_list = pd.DataFrame(positive_list)\n",
    "# print(\"total number: \",len(tweet_list))\n",
    "# print(\"positive number: \",len(positive_list))\n",
    "# print(\"negative number: \", len(negative_list))\n",
    "# print(\"neutral number: \",len(neutral_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Positive [\"+ str(positive)+ \"%]\" , \n",
    "          \"Neutral [\"+ str(neutral)+ \"%]\",\n",
    "          \"Negative [\"+ str(negative)+ \"%]\"]\n",
    "\n",
    "sizes = [positive, neutral, negative]\n",
    "colors = [\"yellowgreen\", \"blue\",\"red\"]\n",
    "patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
    "plt.style.use(\"default\")\n",
    "plt.legend(labels)\n",
    "plt.title(\"Sentiment Analysis Result for keyword= \" +  top_trend_on_twitter_query)\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### BAR GRAPH\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "labels = [\"Positive [\"+ str(positive)+ \"%]\" , \n",
    "          \"Neutral [\"+ str(neutral)+ \"%]\",\n",
    "          \"Negative [\"+ str(negative)+ \"%]\"]\n",
    "\n",
    "sizes = [positive, neutral, negative]\n",
    "colors = [\"yellowgreen\", \"blue\",\"red\"]\n",
    "\n",
    "percentage_ = [float(size) for size in sizes] \n",
    "sns.barplot(x= labels,y=percentage_)\n",
    "plt.style.use(\"default\")\n",
    "plt.legend(labels)\n",
    "plt.title(\"Sentiment Analysis Result for keyword= \" +  top_trend_on_twitter_query)\n",
    "# plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tw_list = pd.DataFrame(tweet_list)\n",
    "tw_list[\"text\"] = tw_list[0]\n",
    "\n",
    "\n",
    "\n",
    "#Removing RT, Punctuation etc\n",
    "remove_rt = lambda x: re.sub(\"RT @\\w+: \",\" \",x)\n",
    "remove_links = lambda x: re.sub(r\"http\\S+\", \"\", x)\n",
    "tw_list[\"text\"] = tw_list.text.map(remove_rt).map(remove_links)\n",
    "tw_list[\"text\"] = tw_list.text.str.lower()\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### SENTIMENT ANALYSIS\n",
    "#Calculating Negative, Positive, Neutral and Compound valuestw_list[[‘polarity’, ‘subjectivity’]] = tw_list[‘text’].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in tw_list[\"text\"].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score[\"neg\"]\n",
    "    neu = score[\"neu\"]\n",
    "    pos = score[\"pos\"]\n",
    "    comp = score[\"compound\"]\n",
    "    if neg > pos:\n",
    "        tw_list.loc[index, \"sentiment\"] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        tw_list.loc[index, \"sentiment\"] = \"positive\"\n",
    "    else:\n",
    "        tw_list.loc[index, \"sentiment\"] = \"neutral\"\n",
    "        tw_list.loc[index, \"neg\"] = neg\n",
    "        tw_list.loc[index, \"neu\"] = neu\n",
    "        tw_list.loc[index, \"pos\"] = pos\n",
    "        tw_list.loc[index, \"compound\"] = comp\n",
    " \n",
    " \n",
    "tw_list.head(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_list_negative = tw_list[tw_list[\"sentiment\"]==\"negative\"]\n",
    "tw_list_positive = tw_list[tw_list[\"sentiment\"]==\"positive\"]\n",
    "tw_list_neutral = tw_list[tw_list[\"sentiment\"]==\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values_in_column(data,feature):\n",
    "    total=data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    "    return pd.concat([total,percentage],axis=1,keys=[\"Total\",\"Percentage\"])#Count_values for sentiment\n",
    "count_values_in_column(tw_list,\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for Pie Chart\n",
    "pichart = count_values_in_column(tw_list,\"sentiment\")\n",
    "names= pichart.index\n",
    "size=pichart[\"Percentage\"]\n",
    " \n",
    "# Create a circle for the center of the plot\n",
    "my_circle=plt.Circle( (0,0), 0.7, color=\"white\")\n",
    "plt.pie(size, labels=names, colors=[\"green\",\"blue\",\"red\"])\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### GETTING ARTICLES AND TEXT MINING\n",
    "\n",
    "################## GET NEWS API SECRET KEYS\n",
    "\n",
    "### LIVERPOOL\n",
    "\n",
    "import pprint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "\n",
    "secret = 'fe92f1b3aa6040159483b4644196ba1b'\n",
    "\n",
    "#### DEFINE THE ENDPOINT\n",
    "url = \"https://newsapi.org/v2/everything?\"\n",
    "\n",
    "\n",
    "\n",
    "# Specify the query and number of returns\n",
    "top_trend_on_twitter_query = \"Liverpool\"\n",
    "parameters = {\n",
    "    'q': top_trend_on_twitter_query, # query phrase\n",
    "    'pageSize': 20,  # maximum is 100\n",
    "    'apiKey': secret,\n",
    "    'from': \"2022-04-19\", # your own API key\n",
    "    'to': \"2022-05-03\" # your own API key\n",
    "}\n",
    "\n",
    "\n",
    "response = requests.get(url, params= parameters)\n",
    "\n",
    "\n",
    "\n",
    "############# GET FIRST NINE ARTICLES\n",
    "articles_list = response.json()[\"articles\"][0:12]\n",
    "\n",
    "\n",
    "article_urls = []\n",
    "\n",
    "articles = []\n",
    "\n",
    "\n",
    "###########  TRANSOFRMATION OF ARTICLES LIST INTO DESIRED FORM\n",
    "for article in articles_list:\n",
    "    articles.append({})\n",
    "\n",
    "    ############# GET CURRENT INDEX TO OPERATE ON, IN THIS CASE IT WOULD BE EMPTY DICT\n",
    "    current_index = len(articles) - 1\n",
    "    \n",
    "    ############# ADD PROPERTIES TO THE EMPTY DIC \n",
    "    articles[current_index][\"newspaper\"] = article[\"source\"][\"name\"] \n",
    "    articles[current_index][\"author\"] = article[\"author\"] \n",
    "    articles[current_index][\"article_title\"] = article[\"title\"] \n",
    "    articles[current_index][\"article_url\"] = article[\"url\"] \n",
    "    articles[current_index][\"date_posted\"] = article[\"publishedAt\"] \n",
    "\n",
    "    article_urls.append(article[\"url\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############## GET INDIVIDUAL ARTICLE FROM ITS RESPECTIVE WEBSITE\n",
    "for index, article in enumerate(articles):\n",
    "    url = article[\"article_url\"]\n",
    "    response = requests.get(url)\n",
    "    soap = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    ind = 0\n",
    "    ############# EXTRACT Paragraph tags from the body\n",
    "    article_description = \"\"\n",
    "    for every_p in soap.select(\"body p\"):\n",
    "        article_description += every_p.getText()\n",
    "        #if not bool(re.search(\"JS ad blocker\", article_description)):\n",
    "        articles[index][\"article_description\"] = article_description\n",
    "         #   ind += ind\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def csv_set_article_header(article):\n",
    "          with open(\"articles.csv\", \"a\", encoding=\"utf-8\",newline=\"\") as file:\n",
    "            header = [key for key in article]\n",
    "            csv_writer = csv.writer(file)\n",
    "            csv_writer.writerow(header)\n",
    "\n",
    "\n",
    "\n",
    "def save_article_to_csv(tweet):\n",
    "            with open(\"articles.csv\", \"a\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "                csv_writer = csv.writer(file)\n",
    "                tweet_content = [tweet[key] for key in tweet]\n",
    "                csv_writer.writerow(tweet_content)\n",
    "                return tweet_content\n",
    "\n",
    "        \n",
    "    #### FUNCTION TO PROCESS WHOLE FETCHING & SAVING OF TWEETS \n",
    "def save_articles(articles):\n",
    "    pattern = r\"Please enable JS and disable any ad blocker|Please|please|blocker|adblocker|ad block|disable\"\n",
    "   \n",
    "    for article in articles:\n",
    "        if not bool(re.search(pattern, article[\"article_description\"], re.IGNORECASE)):\n",
    "            save_article_to_csv(article)\n",
    "\n",
    "\n",
    "\n",
    "csv_set_article_header(articles[0])\n",
    "save_articles(articles=articles)\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### TOPIC MODELING\n",
    "\n",
    "\n",
    "\n",
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(path, file_name):\n",
    "    document_list = []\n",
    "    titles = []\n",
    "\n",
    "    print(os.path.join(path, file_name))\n",
    "    articles_path = Path() / \"articles.csv\"\n",
    "    articles = pd.read_csv(articles_path)\n",
    "\n",
    "    article_descriptions = articles[\"article_description\"].tolist()\n",
    "    article_title = articles[\"article_title\"].tolist()\n",
    "    print(len(article_descriptions))\n",
    "    for article_desc in article_descriptions:\n",
    "        if isinstance(article_desc, str):\n",
    "            #print(article_desc)\n",
    "            text = article_desc.strip()\n",
    "            document_list.append(text)\n",
    "\n",
    "    for article_title in article_title:\n",
    "        titles.append(article_title)       \n",
    "    print(\"total number of documents: \", len(document_list))\n",
    "    \n",
    "    return document_list,titles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pre_process_data(doc_set):\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    en_stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    texts = []\n",
    "\n",
    "    for i in doc_set:\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "        stopped_tokens = [i for i in tokens if i not in en_stop]\n",
    "\n",
    "\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "        texts.append(stemmed_tokens)\n",
    "    \n",
    "    return texts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_corpus(doc_clean):\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "    return dictionary, doc_term_matrix\n",
    "\n",
    "\n",
    "\n",
    "def create_gensim_lsa_model(doc_clean, number_of_topics, words):\n",
    "    dictionary, doc_term_matrix = prepare_corpus(doc_clean)\n",
    "\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word=dictionary)\n",
    "    #print(lsamodel.print_topics(num_topics=number_of_topics, num_words = words) )\n",
    "    return lsamodel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "number_of_topics = 7\n",
    "words = 10\n",
    "document_list,titles =load_data(\"\", \"articles.csv\")\n",
    "clean_texts = pre_process_data(document_list)\n",
    "model = create_gensim_lsa_model(clean_texts,number_of_topics, words)\n",
    "\n",
    "\n",
    "model\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## WORDCLOUD ARTICLES\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def wordcloud_draw(document, stopwords):\n",
    "    tweet_tokens = nltk.word_tokenize(document)\n",
    "    stopwords = set(stopwords)\n",
    "\n",
    "    tweet_tokens\n",
    "\n",
    "\n",
    "    filtered_tweet_text = [w for w in tweet_tokens if w not in stopwords]\n",
    "\n",
    "    wordcloud = WordCloud()\n",
    "\n",
    "    tweet_freq_dist = nltk.FreqDist(filtered_tweet_text)\n",
    "\n",
    "    sorted(tweet_freq_dist,key=tweet_freq_dist.__getitem__,reverse=True)\n",
    "\n",
    "    large_words = dict([(k,v) for (k,v) in tweet_freq_dist.items() if len(k) > 3])\n",
    "\n",
    "    frequency_dist = nltk.FreqDist(large_words)\n",
    "    frequency_dist.plot(30,cumulative=False)\n",
    "\n",
    "    wordcloud = WordCloud(max_font_size=50, max_words=100,background_color=\"white\").generate_from_frequencies(frequency_dist)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "path_to_file = Path() / \"articles.csv\"\n",
    "\n",
    "article_data = pd.read_csv(path_to_file)\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/57237193/delete-rows-in-pandas-given-a-regex\n",
    "\n",
    "m = ~article_data['article_description'].str.contains('JS disable ad blocker ')\n",
    "\n",
    "article_data = article_data[m] \n",
    "articles = article_data[\"article_description\"].to_list()\n",
    "\n",
    "\n",
    "custom_stop_words = [\"this\",\"they\",\"This\",\"That\",\"They\",\"RT\",\"rt\"] \n",
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "for i in custom_stop_words:\n",
    "    stopwords.append(i)\n",
    "for index, document in enumerate(articles):\n",
    "    if isinstance(document,str):\n",
    "        wordcloud_draw(document=document, stopwords=stopwords)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### WORD COUNT OF ARTICLES  & TIME SERIES\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "articles = pd.read_csv(\"articles.csv\")\n",
    "\n",
    "list_of_articles = articles[\"article_description\"].to_list()\n",
    "article_title = articles[\"article_title\"].to_list()\n",
    "articles_date_posted = articles[\"date_posted\"].apply(lambda x : str(datetime.datetime.strftime(parse(x),\"%Y-%m-%d %H:%M:%S\"))).to_list()\n",
    "\n",
    "articles_word_count_list = []\n",
    "\n",
    "for index, article in enumerate(list_of_articles):\n",
    "    article_word_count = len(article.split())\n",
    "    articles_word_count_list.append(article_word_count)\n",
    "    print(article_title[index] + \" \" +\"Word Count: \" + str(article_word_count) + \"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "x = articles_date_posted\n",
    "y = articles_word_count_list\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xticks(rotation=75)\n",
    "plt.title(\"Time Series Of Words Used in Articles Related To Liverpool\")\n",
    "plt.xlabel(\"Time\", size=10)\n",
    "plt.ylabel(\"Word Count\", size=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usama\\Desktop\\BCU\\anaconda\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6884/1138138325.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensimvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m \u001b[0mid2word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;31m#pyLDAvis.save_html(vis, 'lda.html')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "###################### PLOT TOPICS\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as  pd\n",
    "import pyLDAvis \n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv(\"articles.csv\")\n",
    "\n",
    "\n",
    "# CONVERT TO LIST\n",
    "data = df[\"article_description\"].to_list()  \n",
    "\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "  for sentence in sentences:\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))            #deacc=True removes punctuations\n",
    "data_words = list(sent_to_words(data))\n",
    "#print(data_words[:1])\n",
    "\n",
    "\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "# See trigram example\n",
    "#print(trigram_mod[bigram_mod[data_words[0]]])\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stopwords] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "\n",
    "\n",
    "# Create Dictionary \n",
    "id2word = corpora.Dictionary(data_lemmatized)  \n",
    "# Create Corpus \n",
    "texts = data_lemmatized  \n",
    "# Term Document Frequency \n",
    "corpus = [id2word.doc2bow(text) for text in texts]  \n",
    "# View \n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "\n",
    "pyLDAvis.save_html(vis, 'lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### ARTICLE SUMMARISATION\n",
    "#https://medium.com/analytics-vidhya/simple-text-summarization-using-nltk-eedc36ebaaf8\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "\n",
    "articles = pd.read_csv(\"articles.csv\")\n",
    "\n",
    "list_of_article_desc = articles[\"article_description\"].to_list()\n",
    "article = list_of_article_desc[0]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "tokens = word_tokenize(article)\n",
    "sent_token = sent_tokenize(article)\n",
    "\n",
    "word_frequencies = {}\n",
    "for word in tokens:    \n",
    "    if word.lower() not in stop_words:\n",
    "        if word.lower() not in punctuation:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "\n",
    "\n",
    "max_frequency = max(word_frequencies.values())\n",
    "\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = word_frequencies[word]/max_frequency\n",
    "\n",
    "\n",
    "sentence_scores = {}\n",
    "for sent in sent_token:\n",
    "    sentence = sent.split(\" \")\n",
    "    for word in sentence:        \n",
    "        if word.lower() in word_frequencies.keys():\n",
    "            if sent not in sentence_scores.keys():\n",
    "                sentence_scores[sent] = word_frequencies[word.lower()]\n",
    "            else:\n",
    "                sentence_scores[sent] += word_frequencies[word.lower()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "select_length = int(len(sent_token)*0.05)\n",
    "\n",
    "\n",
    "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n",
    "final_summary = [word for word in summary]\n",
    "summary = ' '.join(final_summary)\n",
    "\n",
    "summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:15: DeprecationWarning: invalid escape sequence \\d\n",
      "C:\\Users\\usama\\AppData\\Local\\Temp/ipykernel_6884/1598887390.py:15: DeprecationWarning: invalid escape sequence \\d\n",
      "  article_text = re.sub(\"\\d+\", \"\", article)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "articles = pd.read_csv(\"articles.csv\")\n",
    "articles_description_list = articles[\"article_description\"].to_list()\n",
    "\n",
    "article = articles_description_list[0]\n",
    "\n",
    "\n",
    "article_text = re.sub(\"\\d+\", \"\", article)\n",
    "\n",
    "article_text = re.sub(r'[0-9]', \"\", article_text)\n",
    "article_text = ''.join([i for i in article_text if not i.isdigit()])\n",
    "# print(article_text)\n",
    "article_text = article_text.split(\" \")\n",
    "article_text =  [word for word in article_text if word not in stop_words]\n",
    "# article_text = [word for word in article_text if len(word) > 3]\n",
    "\n",
    "\n",
    "# articles_description_list_ = []\n",
    "# for article in articles_description_list:\n",
    "#     article_text = re.sub(\"[^A-Za-z0-9 ]|[0-9]|[0-9^[a-z]|[0-9$[a-z]]]+\", \"\", article)\n",
    "#     article_text = article_text.split(\" \")\n",
    "#     article_text = [word for word in article_text if len(word) > 3]\n",
    "#     article_text = ' '.join(article_text)\n",
    "#     articles_description_list_.append(article_text)\n",
    "\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "#bag_of_words = count_vectorizer.fit_transform(articles_description_list_)\n",
    "#tfidf = tfidf_vectorizer.fit_transform(articles_description_list_)\n",
    "\n",
    "bag_of_words = count_vectorizer.fit_transform(article_text)\n",
    "tfidf = tfidf_vectorizer.fit_transform(article_text)\n",
    "\n",
    "\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "df_idf = pd.DataFrame(tfidf.toarray(), columns = tfidf_feature_names)\n",
    "df_bow = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)\n",
    "\n",
    "\n",
    "print(df_idf)\n",
    "print(df_bow)\n",
    "\n",
    "#df_bow.to_csv(\"bow.csv\")\n",
    "#df_bow.to_csv(\"tfidf.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2edc8d72492fce6da8c9f093076faead55b47d1ae243b1a3a4b5a0eb61a5ea42"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
